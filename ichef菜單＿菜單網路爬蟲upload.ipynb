{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver(driver_path):\n",
    "    # Get the path for the ChromeDriver\n",
    "    # driver_path = ChromeDriverManager().install()\n",
    "\n",
    "    # Set up Chrome options or capabilities (if needed)\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    # chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-images\")\n",
    "\n",
    "    # Create a Chrome service with the driver path\n",
    "    chrome_service = Service(driver_path)\n",
    "\n",
    "    # Initialize the Chrome WebDriver with options and service\n",
    "    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_menu_item(url, restaurant_id, driver_path):\n",
    "    driver = setup_driver(driver_path)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for the page to load completely (optional, but recommended)\n",
    "        driver.implicitly_wait(60)  # Wait for up to 10 seconds for elements to appear\n",
    "\n",
    "        # Get the store name\n",
    "        wait = WebDriverWait(driver, 30)\n",
    "        store_name_element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'StoreAuthHeader__StoreName-ihWYeu')))\n",
    "        # Get the text of the store name element\n",
    "        store_name = store_name_element.text\n",
    "\n",
    "        # find sections that contain the cat btn\n",
    "        cat_sections = driver.find_elements(By.CSS_SELECTOR, \".OrderingMenuCategoryChip__Wrapper-jaTTMW.dfpPQD\")\n",
    "        cat_btn_lst = [cat_section.find_element(By.CSS_SELECTOR, \"\"\"div[role=\"button\"]\"\"\") for cat_section in cat_sections]\n",
    "\n",
    "        item_list = []\n",
    "        # iterate through cat btns, click the btn then find ele\n",
    "        processed_categories = set()  # Keep track of processed categories\n",
    "        for btn in cat_btn_lst:\n",
    "            btn.click()\n",
    "            time.sleep(1)  # Wait for the page to scroll and load items\n",
    "\n",
    "            # Find all categories in the current section\n",
    "            cats = driver.find_elements(By.CSS_SELECTOR, 'div[data-index]')\n",
    "\n",
    "            for cat in cats:\n",
    "                cat_element = cat.find_element(By.TAG_NAME, 'h4')\n",
    "                cat_name = cat_element.text\n",
    "\n",
    "                if cat_name not in processed_categories:\n",
    "                    processed_categories.add(cat_name)\n",
    "\n",
    "                    # Find all items within the current category\n",
    "                    menu_items = cat.find_elements(By.CSS_SELECTOR, \"li[data-test-id='menuItem']\")\n",
    "                    for item in menu_items:\n",
    "                        # print(item.text)\n",
    "                        item_info = item.text.split('\\n')\n",
    "                        if len(item_info) == 3:\n",
    "                            item_list.append([cat_name, item_info[0], item_info[1], item_info[2], item.text])\n",
    "                        elif len(item_info) == 2:\n",
    "                            item_list.append([cat_name, item_info[0], None, item_info[1], item.text])\n",
    "                        else:\n",
    "                            item_list.append([None, None, None, None, item.text])\n",
    "\n",
    "        print(store_name, item_list)\n",
    "        return store_name, item_list, restaurant_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None, None, restaurant_id\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dish_dict(text_list):\n",
    "    section = text_list[0]\n",
    "    text = text_list[4]\n",
    "    splitted = text.split('\\n')\n",
    "    # 名稱\n",
    "    item_name = splitted[0]\n",
    "    # 價錢\n",
    "    prices = [int(item.replace(\"NT$\", '').replace(',', '')) for item in splitted if item.replace('-', '').startswith('NT$')]\n",
    "    if len(prices) == 1:\n",
    "        org_price = prices[0]\n",
    "        disc_price = prices[0]\n",
    "    elif len(prices) == 2:\n",
    "        org_price = max(prices)\n",
    "        disc_price = min(prices)\n",
    "    # 描述\n",
    "    if text_list[2] and 'NT$' not in text_list[2]:\n",
    "        descrip = text_list[2]\n",
    "    else:\n",
    "        descrip = None\n",
    "    \n",
    "    dish = {\n",
    "        'section': section,\n",
    "        'product': item_name,\n",
    "        'price': org_price,\n",
    "        'discounted_price': disc_price,\n",
    "        'description': descrip\n",
    "    }\n",
    "    return dish\n",
    "\n",
    "def new_menu(menu):\n",
    "    new_menu = []\n",
    "    if menu == [] or menu == None:\n",
    "        return []\n",
    "    for dish in menu:\n",
    "        print(dish)\n",
    "        new_menu.append(dish_dict(dish))\n",
    "    return new_menu\n",
    "\n",
    "# new_menu_list = []\n",
    "# for i in range(uni_df.shape[0]):\n",
    "#     new_menu_list.append(new_menu(uni_df['Items'][i]))\n",
    "\n",
    "# uni_df['new menu'] = new_menu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===Read Restaurant Link===\n",
    "df = pd.read_json(\"restaurant_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver (outside the scraping function)\n",
    "driver_path = ChromeDriverManager().install()\n",
    "\n",
    "store_name_list, item_list, id_list, time_list = [], [], [], []\n",
    "# Function to process a chunk of URLs\n",
    "def process_chunk(start_index, end_index):\n",
    "    for a_store in range(start_index, end_index):\n",
    "        # get current time\n",
    "        current_time = datetime.now()\n",
    "        # scrape\n",
    "        print(f\"The {a_store} store\")\n",
    "        url, restaurant_id = df['link'][a_store], df['id'][a_store] # Target df with url\n",
    "        store_name, items, id = scrape_menu_item(url, restaurant_id, driver_path)\n",
    "        store_name_list.append(store_name)\n",
    "        item_list.append(items)\n",
    "        id_list.append(id)\n",
    "        time_list.append(current_time)\n",
    "\n",
    "# Divide URLs into chunks and process them in parallel\n",
    "#=========\n",
    "chunk_size = 100  # Adjust chunk size as needed\n",
    "start = 0\n",
    "num_urls = df.shape[0]\n",
    "#=========\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = []\n",
    "    for start_index in range(start, num_urls, chunk_size):\n",
    "        end_index = min(start_index + chunk_size, num_urls)\n",
    "        futures.append(executor.submit(process_chunk, start_index, end_index))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Create a new DataFrame to store the scraped data\n",
    "scraped_data = pd.DataFrame({\n",
    "    'Store_name': store_name_list,\n",
    "    'Items': item_list,\n",
    "    'id': id_list,\n",
    "    'update_time': time_list\n",
    "})\n",
    "\n",
    "# Merge the scraped data with the original DataFrame df based on index or any other common column\n",
    "# df2 = pd.concat([df2, scraped_data], axis=0)\n",
    "\n",
    "# Save DataFrame to JSON\n",
    "# df2.to_json('restaurant_menu.json', orient='records')\n",
    "\n",
    "scraped_data.to_json('restaurant_menu0425.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_df = scraped_data.drop_duplicates(subset='id')\n",
    "uni_df.reset_index(drop=True, inplace=True)\n",
    "uni_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===Merge to the original df===\n",
    "merged_df = pd.merge(df, uni_df, on = 'id', how = 'left')\n",
    "null_rows = merged_df[merged_df['Store_name'].isnull()]\n",
    "not_scraped_list = null_rows.index\n",
    "merged_df = merged_df.drop_duplicates(subset='id')\n",
    "merged_df.to_csv('restaurant_menu0418.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===For those with error===\n",
    "re_scrape_df = merged_df[merged_df['Store_name'].isnull()].reset_index(drop=True)\n",
    "re_scrape_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===Re Scrape===\n",
    "# Initialize the Chrome driver (outside the scraping function)\n",
    "driver_path = ChromeDriverManager().install()\n",
    "\n",
    "store_name_list, item_list, id_list, time_list = [], [], [], []\n",
    "# Function to process a chunk of URLs\n",
    "def process_chunk(start_index, end_index):\n",
    "    for a_store in range(start_index, end_index):\n",
    "        # get current time\n",
    "        current_time = datetime.now()\n",
    "        # scrape\n",
    "        print(f\"The {a_store} store\")\n",
    "        url, restaurant_id = re_scrape_df['link'][a_store], re_scrape_df['id'][a_store] # Target df with url\n",
    "        store_name, items, id = scrape_menu_item(url, restaurant_id)\n",
    "        store_name_list.append(store_name)\n",
    "        item_list.append(items)\n",
    "        id_list.append(id)\n",
    "        time_list.append(current_time)\n",
    "\n",
    "# Divide URLs into chunks and process them in parallel\n",
    "#=========\n",
    "chunk_size = 50  # Adjust chunk size as needed\n",
    "start = 0\n",
    "num_urls = re_scrape_df.shape[0]\n",
    "#=========\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = []\n",
    "    for start_index in range(start, num_urls, chunk_size):\n",
    "        end_index = min(start_index + chunk_size, num_urls)\n",
    "        futures.append(executor.submit(process_chunk, start_index, end_index))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Create a new DataFrame to store the scraped data\n",
    "rescraped_data = pd.DataFrame({\n",
    "    'Store_name': store_name_list,\n",
    "    'Items': item_list,\n",
    "    'id': id_list,\n",
    "    'update_time': time_list\n",
    "})\n",
    "\n",
    "# Merge the scraped data with the original DataFrame df based on index or any other common column\n",
    "scraped_data = pd.concat([scraped_data, rescraped_data], axis=0)\n",
    "\n",
    "# Save DataFrame to JSON\n",
    "scraped_data.to_json('restaurant_menu0418.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_df = scraped_data.drop_duplicates(subset='id')\n",
    "uni_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "new_menu_list = []\n",
    "for i in range(uni_df.shape[0]):\n",
    "    new_menu_list.append(new_menu(uni_df['Items'][i]))\n",
    "\n",
    "uni_df['new menu'] = new_menu_list\n",
    "\n",
    "# ===Merge to the original df===\n",
    "merged_df = pd.merge(df, uni_df, on = 'id', how = 'left')\n",
    "null_rows = merged_df[merged_df['Store_name'].isnull()]\n",
    "not_scraped_list = null_rows.index\n",
    "merged_df = merged_df.drop_duplicates(subset='id')\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "merged_df.to_csv('restaurant_menu0418.csv', index = False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dish_dict(text_list):\n",
    "    section = text_list[0]\n",
    "    text = text_list[4]\n",
    "    splitted = text.split('\\n')\n",
    "    # 名稱\n",
    "    item_name = splitted[0]\n",
    "    # 價錢\n",
    "    prices = [int(item.replace(\"NT$\", '').replace(',', '')) for item in splitted if item.replace('-', '').startswith('NT$')]\n",
    "    if len(prices) == 1:\n",
    "        org_price = prices[0]\n",
    "        disc_price = prices[0]\n",
    "    elif len(prices) == 2:\n",
    "        org_price = max(prices)\n",
    "        disc_price = min(prices)\n",
    "    # 描述\n",
    "    if text_list[2] and 'NT$' not in text_list[2]:\n",
    "        descrip = text_list[2]\n",
    "    else:\n",
    "        descrip = None\n",
    "    \n",
    "    dish = {\n",
    "        'section': section,\n",
    "        'product': item_name,\n",
    "        'price': org_price,\n",
    "        'discounted_price': disc_price,\n",
    "        'description': descrip\n",
    "    }\n",
    "    return dish\n",
    "\n",
    "def new_menu(menu):\n",
    "    new_menu = []\n",
    "    if menu == [] or menu == None:\n",
    "        return []\n",
    "    for dish in menu:\n",
    "        print(dish)\n",
    "        new_menu.append(dish_dict(dish))\n",
    "    return new_menu\n",
    "\n",
    "# new_menu_list = []\n",
    "# for i in range(uni_df.shape[0]):\n",
    "#     new_menu_list.append(new_menu(uni_df['Items'][i]))\n",
    "\n",
    "# uni_df['new menu'] = new_menu_list"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
